---
title: Week 32
description: Week 32
header: Week 32
---

# Presentation notes pt.1
The 2018 KU symposium is in two weeks, so I decided to start outlining my presentation. I only have 10 minutes to talk, so again the goal is to make the research as simple and as profound as possible. You know, high-level messaging through simple clean digestible powerpoint slides.

In fact, I've taken my [speech concept](https://eoduniyi.github.io/MLL-blog/Week9) slides and [CogSci 2018 outline](https://eoduniyi.github.io/MLL-blog/Week13) slides and melded them together to give myself a nice outline. I actually presented these slides to Dr. Brumberg and Rebekah this week. Though, because they've already seen most of these slides, we mostly talked about what I should and shouldn't present. For example, avoid talking about how computing systems (artificial intelligence) is and will continue to manage a weird amount of human life...


![slide1](https://storage.googleapis.com/root-proposal-1246/CREU_DATA/week_32/Slide1.png)

The Amazon Echo (Alexa) is a great example of a modern day speech recognition system. These systems help us play music, make reservations, and schedule events in our lives.


![slide2](https://storage.googleapis.com/root-proposal-1246/CREU_DATA/week_32/Slide2.png)

These speech recognition systems are pretty robust and typically human-to-computer speech recognition is pretty accurate. That is, most of the time Alexa will recognize your speech.

![slide3](https://storage.googleapis.com/root-proposal-1246/CREU_DATA/week_32/Slide3.png)

Within all speech recognition system is the decoder. The decoder is thing doing the heavy lifting.

![slide4](https://storage.googleapis.com/root-proposal-1246/CREU_DATA/week_32/Slide4.png)

On further inspection, the task of the decoder is to find the string of words that maximizes the likelihood of generating the observation. That is, what is the probability of these set of words generating said speech.


![slide5](https://storage.googleapis.com/root-proposal-1246/CREU_DATA/week_32/Slide5.png)

Perhaps children also have a statistical model of speech.


![slide6](https://storage.googleapis.com/root-proposal-1246/CREU_DATA/week_32/Slide6.png)

Now, we want to highlight the language representation that humans use when decoding (perceiving) speech. We use networks to represent language (complex system).


![slide7](https://storage.googleapis.com/root-proposal-1246/CREU_DATA/week_32/Slide7.png)

This allows us to highlight how the adult's language network is much larger than a child's. The interpretation here is that the child knows less words and the connections between these words aren't as redundant or even efficient for navigating.


![slide8](https://storage.googleapis.com/root-proposal-1246/CREU_DATA/week_32/Slide8.png)

High-level messaging :)


![slide9](https://storage.googleapis.com/root-proposal-1246/CREU_DATA/week_32/Slide9.png)

Okay, so what we want to do is see how an automatic speech recognition system will learn given a ton of child-directed speech. Basically, we swap the "typical" American child for Alexa to model both nature (implicit knowledge) and nurture (instruction by guardians).


Best, <br />
EO
